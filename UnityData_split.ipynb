{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "s3YBDAS4ccvC"
   },
   "source": [
    "For Mask-RCNN\n",
    "用來重新排序及分trainingdata , testingdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUtTAZefVWai"
   },
   "source": [
    "## Define a data converter for data collected by Unity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CCNtjbCjWhsr"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import random\n",
    "import shutil\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "import annotated_images\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "cuASsFt6DPHC"
   },
   "outputs": [],
   "source": [
    "class UnityDataConverter():\n",
    "  def __init__(self, data_dir):\n",
    "    self.data_dir = data_dir\n",
    "    \n",
    "  def delete_outlier(self):\n",
    "    '''\n",
    "    delete first image and put all into same file\n",
    "    '''\n",
    "    scenes = sorted(os.listdir(self.data_dir))\n",
    "    \n",
    "    for scene in scenes:      \n",
    "        target_jpg = self.data_dir+'/'+ scene +'/'+'1.main.jpg'\n",
    "        target_png = self.data_dir+'/'+ scene +'/'+'1.main.seg.png'\n",
    "        target_json = self.data_dir+'/'+ scene +'/'+'1.main.json'\n",
    "        target_depth = self.data_dir+'/'+ scene +'/'+'1.main.depth.png'\n",
    "        src = self.data_dir+'/'+ scene\n",
    "        try:\n",
    "            os.remove(target_jpg)\n",
    "            print(f'\\'delete\\':{target_jpg}')                  \n",
    "        except:\n",
    "            print(f'{target_jpg} not found')\n",
    "                \n",
    "        try:\n",
    "            os.remove(target_png)\n",
    "            print(f'\\'delete\\':{target_png}')                  \n",
    "        except:\n",
    "            print(f'{target_png} not found')\n",
    "            \n",
    "        try:\n",
    "            os.remove(target_json)\n",
    "            print(f'\\'delete\\':{target_json}')                  \n",
    "        except:\n",
    "            print(f'{target_json} not found')\n",
    "            \n",
    "        try:\n",
    "            os.remove(target_depth)\n",
    "            print(f'\\'delete\\':{target_depth}')                  \n",
    "        except:\n",
    "            print(f'{target_depth} not found')\n",
    "        \n",
    "        images_path_jpg = sorted(glob(os.path.join(self.data_dir+'/'+scene+'/*.main.jpg')))\n",
    "        images_path_png = sorted(glob(self.data_dir+'/'+scene+'/*.main.seg.png'))\n",
    "        json_path = sorted(glob(self.data_dir+'/'+scene+'/*.main.json'))     \n",
    "        depth_path = sorted(glob(self.data_dir+'/'+scene+'/*.main.depth.png'))\n",
    "        \n",
    "        print(len(images_path_jpg))\n",
    "        \n",
    "        for i in range (len(images_path_jpg)):\n",
    "            \n",
    "            folder_name = self.data_dir+'/'+scene+ '/' + scene + '_'+ str(i) \n",
    "            src_jpg = os.path.join(folder_name +'.jpg')\n",
    "            src_png = os.path.join(folder_name +'.main.seg.png')\n",
    "            src_json = os.path.join(folder_name +'.main.json')\n",
    "            src_depth = os.path.join(folder_name +'.main.depth.png')\n",
    "            \n",
    "#           print(f'image:{images_path_jpg[i]} /src_jpg:{src_jpg}')\n",
    "            os.rename(images_path_jpg[i],src_jpg)\n",
    "            os.rename(images_path_png[i],src_png)\n",
    "            os.rename(json_path[i],src_json)\n",
    "            os.rename(depth_path[i],src_depth)\n",
    "        \n",
    "        dst = './all_image/'\n",
    "        if not os.path.exists('./all_image'):\n",
    "            os.mkdir('./all_image')\n",
    "            \n",
    "            \n",
    "        shutil.copytree(src, dst,dirs_exist_ok=True)\n",
    "              \n",
    "  def split_train_test_val(self, input_dir = './all_image', dst = './all_image/split_dir'):\n",
    "    train_dst = dst+'/train'\n",
    "    test_dst = dst+'/val'\n",
    "    \n",
    "    if not os.path.exists(dst):\n",
    "        os.mkdir(dst)\n",
    "        os.mkdir(train_dst)\n",
    "        os.mkdir(test_dst)\n",
    "                           \n",
    "    all_file = glob(os.path.join( input_dir + '/*.jpg'))\n",
    "    #print(all_file)\n",
    "    \n",
    "    training_data, testing_data = train_test_split(all_file, test_size=0.2, random_state=0)\n",
    "    \n",
    "    for i in range (len(training_data)):\n",
    "        shutil.move(training_data[i], train_dst)\n",
    "    for i in range (len(testing_data)):\n",
    "        shutil.move(testing_data[i], test_dst)\n",
    "                           \n",
    "    train_num = len(training_data)\n",
    "    val_num = len(testing_data)\n",
    "    tatol_num = train_num + val_num \n",
    "\n",
    "    print(\"Split successfully\")\n",
    "    print(f'\\'train\\': {train_num},\\'val\\': {val_num}, \\'total\\': {tatol_num}')\n",
    "    \n",
    "  def process_seg_img(self,rootdir = './all_image/split_dir/val'):\n",
    "     \n",
    "    #get the file name in split folder \n",
    "    #rootdir = './all_image/split_dir/val'\n",
    "    #rootdir = './all_image/split_dir/train'\n",
    "    \n",
    "    width = 640\n",
    "    height = 480\n",
    "    seg_file = rootdir +'.json'  # store info\n",
    "    json_path = []\n",
    "    seg_path = []\n",
    "    image_list = []\n",
    "    annotation_list = []\n",
    "    file_name_path = []\n",
    "    null= None\n",
    "    \n",
    "    for rootdir, dirs, files in os.walk(rootdir):\n",
    "        print('done') \n",
    "        #print(files) #files: image\n",
    "        \n",
    "    for i in range(len(files)):\n",
    "        basename = os.path.basename(files[i])\n",
    "        file_name = os.path.splitext(basename)[0]\n",
    "        file_name_path.append(file_name)\n",
    "        json_path.append('./all_image/'+ file_name + '.main.json')\n",
    "        seg_path.append('./all_image/'+ file_name+ '.main.seg.png')\n",
    "      \n",
    "    num_annos = 0  # Number of annotations\n",
    "    num_imgs = 0 # Number of images\n",
    "    \n",
    "    for i in range (len(json_path)):\n",
    "        \n",
    "    # Segementation   \n",
    "         ###read json   \n",
    "        with open(json_path[i], 'r') as load_f:\n",
    "            dataset= []\n",
    "            load_dict = json.load(load_f)\n",
    "            objs = load_dict['objects']\n",
    "            for obj in objs:\n",
    "                name = obj['class']\n",
    "                bbox = obj['bounding_box']\n",
    "                xmin = bbox['top_left'][0]\n",
    "                ymin = bbox['top_left'][1]\n",
    "                xmax = bbox['bottom_right'][0]\n",
    "                ymax = bbox['bottom_right'][1]\n",
    "                # print(xmin, ymin, xmax, ymax)\n",
    "                if xmin < 0: xmin = 0\n",
    "                if xmax < 0: xmax = 0\n",
    "                if ymin < 0: ymin = 0\n",
    "                if ymax < 0: ymax = 0\n",
    "                if xmin > width: xmin = 640 #width\n",
    "                if xmax > width: xmax = 640 #width\n",
    "                if ymin > height: ymin = 480 #height\n",
    "                if ymax > height: ymax = 480 #height\n",
    "                x_lenth = xmax-xmin\n",
    "                y_lenth = ymax-ymin\n",
    "                dataset.append(xmin)\n",
    "                dataset.append(ymin)\n",
    "                dataset.append(x_lenth)\n",
    "                dataset.append(y_lenth)\n",
    "                \n",
    "                if name == 'WAM_V':\n",
    "                    category = 1\n",
    "                elif name == 'Boat':\n",
    "                    category = 2\n",
    "                elif name == 'BigBoat':\n",
    "                    category = 3\n",
    "                elif name == null :\n",
    "                    category = 0\n",
    "            ###      \n",
    "            img = cv2.imread(seg_path[i])\n",
    "            array = np.array(img) \n",
    "            array_new = array  \n",
    "            cnt=0\n",
    "            for j in range (480):\n",
    "                for k in range (640):\n",
    "                    for l in range (3) :\n",
    "                        if array_new[j][k][l] !=0 :\n",
    "                            array_new[j][k][l] = 255\n",
    "                            cnt+=1\n",
    "                        else : \n",
    "                            pass      \n",
    "            image_order = (re.findall(r\"\\d+\",seg_path[i]))[1]\n",
    "            #cv2.imwrite(self.data_dir+'/'+scene+'/'+image_order+'.new_main.seg.png', array_new)\n",
    "            #mask_img = cv2.imread(self.data_dir+'/'+scene+'/'+ image_order +'.new_main.seg.png') \n",
    "            \n",
    "            cv2.imwrite('./all_image/'+ file_name_path[i] +'.new_main.seg.png', array_new)\n",
    "            mask_img = cv2.imread('./all_image/'+ file_name_path[i] +'.new_main.seg.png') \n",
    "            \n",
    "            #cv2_imshow(mask_img)     \n",
    "            gray = cv2.cvtColor(mask_img,cv2.COLOR_BGR2GRAY)  \n",
    "            ret, binary = cv2.threshold(gray,0,255,cv2.THRESH_BINARY)  \n",
    "            contours, hierarchy = cv2.findContours(binary,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)  \n",
    "            new_contours = []\n",
    "            \n",
    "            for j in range (len(contours)):\n",
    "                new_contours.extend(contours[j].flatten().tolist())\n",
    "        \n",
    "            image_dict ={\"width\": 640 ,\n",
    "                              \"date_captured\":null , \n",
    "                              \"license\": 0, \n",
    "                              \"url\": null , \n",
    "                              \"file_name\":files[i],\n",
    "                               \"id\": i , \n",
    "                               \"height\": 480}\n",
    "            annotation_dict = {\"image_id\": i,\n",
    "                                \"segmentation\":[new_contours], \n",
    "                                \"bbox\": dataset,\n",
    "                                \"area\": cnt,\n",
    "                                \"iscrowd\": 0,\n",
    "                                \"category_id\" : category,\n",
    "                                \"id\":i }\n",
    "                \n",
    "            image_list.append(image_dict)\n",
    "            annotation_list.append(annotation_dict)\n",
    "            \n",
    "    with open(seg_file, 'w') as f: \n",
    "        final_dic = {\"info\": {\"contributor\": null, \"version\": null, \"description\": null, \n",
    "                     \"year\": 2023, \"url\": null, \"date_created\": \"2023-03-18 00:52:14.378127\"},\n",
    "                     \"images\": image_list,\n",
    "                     \"licenses\": [{\"url\": null, \"name\": null, \"id\": 0}], \n",
    "                     \"type\": \"instances\", \n",
    "                     \"annotations\": annotation_list,\n",
    "                      \"categories\": [{\"supercategory\": null, \"name\": \"_background_\", \"id\": 0}, \n",
    "                                     {\"supercategory\": null, \"name\": \"WAM_V\", \"id\": 1},\n",
    "                                     {\"supercategory\": null, \"name\": \"Boat\", \"id\": 2},\n",
    "                                     {\"supercategory\": null, \"name\": \"BigBoat\", \"id\": 3}\n",
    "                                    ]\n",
    "                    }\n",
    "        \n",
    "        json.dump(final_dic, f) \n",
    "    print('finished')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'delete':./wamv_boat_bigboat/Scene1/1.main.jpg\n",
      "'delete':./wamv_boat_bigboat/Scene1/1.main.seg.png\n",
      "'delete':./wamv_boat_bigboat/Scene1/1.main.json\n",
      "'delete':./wamv_boat_bigboat/Scene1/1.main.depth.png\n",
      "200\n",
      "'delete':./wamv_boat_bigboat/Scene2/1.main.jpg\n",
      "'delete':./wamv_boat_bigboat/Scene2/1.main.seg.png\n",
      "'delete':./wamv_boat_bigboat/Scene2/1.main.json\n",
      "'delete':./wamv_boat_bigboat/Scene2/1.main.depth.png\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "data_dir = './wamv_boat_bigboat'\n",
    "converter = UnityDataConverter(data_dir)\n",
    "converter.delete_outlier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split training/testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split successfully\n",
      "'train': 320,'val': 80, 'total': 400\n"
     ]
    }
   ],
   "source": [
    "converter.split_train_test_val(input_dir = './all_image', dst = './all_image/split_dir')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29Uw_353n-ts"
   },
   "source": [
    "## Create converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cp3-po46Vq93",
    "outputId": "a8caf88d-84a8-4d14-f729-94f3c5344bf0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "data_dir = './wamv_boat_bigboat'\n",
    "converter = UnityDataConverter(data_dir)\n",
    "rootdir = './all_image/split_dir/val'\n",
    "converter.process_seg_img(rootdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir = './all_image/split_dir/train'\n",
    "converter.process_seg_img(rootdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
